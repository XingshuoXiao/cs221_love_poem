# Stanford CS 221 Final Project - Love Poem Generator AI

## Colab Notebook
- ```baseline_encoded.ipynb```: baseline implementation
- ```Finetuned_GPT2.ipynb```: main approach implementation

## Datasets
- ```lovepoem200.txt```: training data collected from [poets.org](https://poets.org/)
- ```lovepoem25.txt```: validation data collected from [poets.org](https://poets.org/)
- ```lovepoem25_old.txt```: validation data collected from [AllPoetry](https://allpoetry.com/)
- ```lovepoem10.txt```: 10 poems extracting from ```lovepoem200.txt``` for perplexity computing
- ```lovepoem_baseline.txt```: training and validation data for baseline
- ```oracle10.txt```: 10 poems generated by [ChatGPT](https://chat.openai.com/)
- ```baseline5.txt```: 5 poems generated by baseline


## Acknowledgments
Our code is inspired by [LSTM-building](https://medium.com/analytics-vidhya/ai-writing-poems-building-lstm-model-using-pytorch-d1c58a24bb64) and [Finetuning-gpt2](https://www.kaggle.com/code/changyeop/how-to-fine-tune-gpt-2-for-beginners).

