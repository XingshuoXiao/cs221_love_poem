{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1eaeYwVXzNMpHOaaPyeXAZbe0ziIYsOJJ","authorship_tag":"ABX9TyNH/ekxrJIExboMBZIDr/QC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"LRawBJ__swig","executionInfo":{"status":"ok","timestamp":1701824034976,"user_tz":480,"elapsed":4906,"user":{"displayName":"Xingshuo Xiao","userId":"07626625558595921239"}}},"outputs":[],"source":["import numpy as np\n","import torch\n","from torch import nn\n","import torch.nn.functional as F"]},{"cell_type":"code","source":["file_path = '/content/drive/Shareddrives/LovePoem/lovepoem_baseline.txt'\n","with open(file_path, 'r', encoding='utf-8') as file:\n","    poems = file.read()"],"metadata":{"id":"z9KG59O5tIdu","executionInfo":{"status":"ok","timestamp":1701824040575,"user_tz":480,"elapsed":961,"user":{"displayName":"Xingshuo Xiao","userId":"07626625558595921239"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["print(poems[:200])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kLx3yrRfuxQO","executionInfo":{"status":"ok","timestamp":1701824041651,"user_tz":480,"elapsed":4,"user":{"displayName":"Xingshuo Xiao","userId":"07626625558595921239"}},"outputId":"a9d052f9-e181-4b9e-d588-fbca8414a81e"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["A million stars up in the sky\n","One shines brighter   I can't deny\n","A love so precious a love so true\n","a love that comes from me to you\n","The angels sing when you are near\n","Within your arms I have nothing to\n"]}]},{"cell_type":"code","source":["chars = tuple(set(poems))\n","int2char = dict(enumerate(chars))\n","char2int = {ch:ii for ii, ch in int2char.items()}\n","encoded = np.array([char2int[i] for i in poems])"],"metadata":{"id":"VYMaLqL-vCxr","executionInfo":{"status":"ok","timestamp":1701824043154,"user_tz":480,"elapsed":173,"user":{"displayName":"Xingshuo Xiao","userId":"07626625558595921239"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def one_hot_encode(arr, n_labels):\n","\n","    # Initialize the the encoded array\n","    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n","\n","    # Fill the appropriate elements with ones\n","    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n","\n","    # Finally reshape it to get back to the original array\n","    one_hot = one_hot.reshape((*arr.shape, n_labels))\n","\n","    return one_hot"],"metadata":{"id":"S8h3wixzyqwk","executionInfo":{"status":"ok","timestamp":1701824044201,"user_tz":480,"elapsed":1,"user":{"displayName":"Xingshuo Xiao","userId":"07626625558595921239"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["test_seq = np.array([[0, 5, 1]])\n","one_hot = one_hot_encode(test_seq, 8)\n","print(one_hot)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fXQWQtEhVUcr","executionInfo":{"status":"ok","timestamp":1701824045501,"user_tz":480,"elapsed":7,"user":{"displayName":"Xingshuo Xiao","userId":"07626625558595921239"}},"outputId":"5ea88c58-6230-40c2-e085-609b27b4dab6"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["[[[1. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 1. 0. 0.]\n","  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"]}]},{"cell_type":"code","source":["def get_batches(arr, batch_size, seq_length):\n","    '''Create a generator that returns batches of size\n","       batch_size x seq_length from arr.\n","\n","       Arguments\n","       ---------\n","       arr: Array you want to make batches from\n","       batch_size: Batch size, the number of sequences per batch\n","       seq_length: Number of encoded chars in a sequence\n","    '''\n","\n","    batch_size_total = batch_size * seq_length\n","    # total number of batches we can make\n","    n_batches = len(arr)//batch_size_total\n","\n","    # Keep only enough characters to make full batches\n","    arr = arr[:n_batches * batch_size_total]\n","    # Reshape into batch_size rows\n","    arr = arr.reshape((batch_size, -1))\n","\n","    # iterate through the array, one sequence at a time\n","    for n in range(0, arr.shape[1], seq_length):\n","        # The features\n","        x = arr[:, n:n+seq_length]\n","        # The targets, shifted by one\n","        y = np.zeros_like(x)\n","        try:\n","            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n","        except IndexError:\n","            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n","        yield x, y"],"metadata":{"id":"qPijELVjWDzF","executionInfo":{"status":"ok","timestamp":1701824046518,"user_tz":480,"elapsed":213,"user":{"displayName":"Xingshuo Xiao","userId":"07626625558595921239"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["train_on_gpu = torch.cuda.is_available()\n","class CharRNN(nn.Module):\n","\n","    def __init__(self, tokens, n_hidden=256, n_layers=2,\n","                               drop_prob=0.5, lr=0.001):\n","        super().__init__()\n","        self.drop_prob = drop_prob\n","        self.n_layers = n_layers\n","        self.n_hidden = n_hidden\n","        self.lr = lr\n","\n","        # creating character dictionaries\n","        self.chars = tokens\n","        self.int2char = dict(enumerate(self.chars))\n","        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n","\n","        #lstm layer\n","        self.lstm=nn.LSTM(len(self.chars),n_hidden,n_layers,\n","                          dropout=drop_prob,batch_first=True)\n","\n","        #dropout layer\n","        self.dropout=nn.Dropout(drop_prob)\n","\n","        #output layer\n","        self.fc=nn.Linear(n_hidden,len(self.chars))\n","\n","    def forward(self, x, hidden):\n","        ''' Forward pass through the network.\n","            These inputs are x, and the hidden/cell state `hidden`. '''\n","        ## Get the outputs and the new hidden state from the lstm\n","        r_output, hidden = self.lstm(x, hidden)\n","\n","        ## pass through a dropout layer\n","        out = self.dropout(r_output)\n","\n","        # Stack up LSTM outputs using view\n","        # you may need to use contiguous to reshape the output\n","        out = out.contiguous().view(-1, self.n_hidden)\n","\n","        ## put x through the fully-connected layer\n","        out = self.fc(out)\n","        return out, hidden\n","\n","\n","    def init_hidden(self, batch_size):\n","        ''' Initializes hidden state '''\n","        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n","        # initialized to zero, for hidden state and cell state of LSTM\n","        weight = next(self.parameters()).data\n","\n","        if (train_on_gpu):\n","            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n","                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n","        else:\n","            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n","                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n","\n","        return hidden"],"metadata":{"id":"GpKLJyPQWVob","executionInfo":{"status":"ok","timestamp":1701824469943,"user_tz":480,"elapsed":242,"user":{"displayName":"Xingshuo Xiao","userId":"07626625558595921239"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n","    ''' Training a network\n","\n","        Arguments\n","        ---------\n","\n","        net: CharRNN network\n","        data: text data to train the network\n","        epochs: Number of epochs to train\n","        batch_size: Number of mini-sequences per mini-batch, aka batch size\n","        seq_length: Number of character steps per mini-batch\n","        lr: learning rate\n","        clip: gradient clipping\n","        val_frac: Fraction of data to hold out for validation\n","        print_every: Number of steps for printing training and validation loss\n","\n","    '''\n","    net.train()\n","\n","    opt = torch.optim.Adam(net.parameters(), lr=lr)\n","    criterion = nn.CrossEntropyLoss()\n","\n","    # create training and validation data\n","    val_idx = int(len(data)*(1-val_frac))\n","    data, val_data = data[:val_idx], data[val_idx:]\n","\n","    if(train_on_gpu):\n","        net.cuda()\n","\n","    counter = 0\n","    n_chars = len(net.chars)\n","    for e in range(epochs):\n","        # initialize hidden state\n","        h = net.init_hidden(batch_size)\n","\n","        for x, y in get_batches(data, batch_size, seq_length):\n","            counter += 1\n","\n","            # One-hot encode our data and make them Torch tensors\n","            x = one_hot_encode(x, n_chars)\n","            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n","\n","            if(train_on_gpu):\n","                inputs, targets = inputs.cuda(), targets.cuda()\n","            # Creating new variables for the hidden state, otherwise\n","            # we'd backprop through the entire training history\n","            h = tuple([each.data for each in h])\n","            # zero accumulated gradients\n","            net.zero_grad()\n","\n","            # get the output from the model\n","            output, h = net(inputs, h)\n","\n","            # calculate the loss and perform backprop\n","            loss = criterion(output, targets.view(batch_size*seq_length).long())\n","            loss.backward()\n","            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n","            nn.utils.clip_grad_norm_(net.parameters(), clip)\n","            opt.step()\n","\n","            # loss stats\n","            if counter % print_every == 0:\n","                # Get validation loss\n","                val_h = net.init_hidden(batch_size)\n","                val_losses = []\n","                net.eval()\n","                for x, y in get_batches(val_data, batch_size, seq_length):\n","                    # One-hot encode our data and make them Torch tensors\n","                    x = one_hot_encode(x, n_chars)\n","                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n","\n","                    # Creating new variables for the hidden state, otherwise\n","                    # we'd backprop through the entire training history\n","                    val_h = tuple([each.data for each in val_h])\n","\n","                    inputs, targets = x, y\n","                    if(train_on_gpu):\n","                        inputs, targets = inputs.cuda(), targets.cuda()\n","                    output, val_h = net(inputs, val_h)\n","                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n","\n","                    val_losses.append(val_loss.item())\n","\n","                net.train() # reset to train mode after iterationg through validation data\n","\n","                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n","                      \"Step: {}...\".format(counter),\n","                      \"Loss: {:.4f}...\".format(loss.item()),\n","                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"],"metadata":{"id":"1v4Ja3VuYjQ4","executionInfo":{"status":"ok","timestamp":1701824409379,"user_tz":480,"elapsed":220,"user":{"displayName":"Xingshuo Xiao","userId":"07626625558595921239"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# define and print the net\n","n_hidden = 256\n","n_layers = 2\n","net = CharRNN(chars, n_hidden, n_layers, drop_prob=0.5)\n","print(net)\n","batch_size = 25\n","seq_length = 50\n","n_epochs =  30\n","# train the model\n","train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, val_frac=0.12, print_every=100)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u9Cb3aBCgp5G","executionInfo":{"status":"ok","timestamp":1701824622674,"user_tz":480,"elapsed":16384,"user":{"displayName":"Xingshuo Xiao","userId":"07626625558595921239"}},"outputId":"2f32bbf3-e630-46ee-abd2-1b272fbd6536"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["CharRNN(\n","  (lstm): LSTM(72, 256, num_layers=2, batch_first=True, dropout=0.5)\n","  (dropout): Dropout(p=0.5, inplace=False)\n","  (fc): Linear(in_features=256, out_features=72, bias=True)\n",")\n","Epoch: 2/30... Step: 100... Loss: 3.0678... Val Loss: 3.0786\n","Epoch: 3/30... Step: 200... Loss: 2.6642... Val Loss: 2.6415\n","Epoch: 4/30... Step: 300... Loss: 2.4926... Val Loss: 2.3557\n","Epoch: 5/30... Step: 400... Loss: 2.2000... Val Loss: 2.2270\n","Epoch: 7/30... Step: 500... Loss: 2.2513... Val Loss: 2.1563\n","Epoch: 8/30... Step: 600... Loss: 2.1307... Val Loss: 2.0932\n","Epoch: 9/30... Step: 700... Loss: 2.1090... Val Loss: 2.0596\n","Epoch: 10/30... Step: 800... Loss: 1.9575... Val Loss: 2.0038\n","Epoch: 12/30... Step: 900... Loss: 2.0175... Val Loss: 1.9902\n","Epoch: 13/30... Step: 1000... Loss: 1.9700... Val Loss: 1.9528\n","Epoch: 14/30... Step: 1100... Loss: 1.9989... Val Loss: 1.9285\n","Epoch: 15/30... Step: 1200... Loss: 1.8185... Val Loss: 1.9095\n","Epoch: 17/30... Step: 1300... Loss: 1.9247... Val Loss: 1.8976\n","Epoch: 18/30... Step: 1400... Loss: 1.8856... Val Loss: 1.8753\n","Epoch: 19/30... Step: 1500... Loss: 1.8855... Val Loss: 1.8581\n","Epoch: 20/30... Step: 1600... Loss: 1.7420... Val Loss: 1.8486\n","Epoch: 22/30... Step: 1700... Loss: 1.8349... Val Loss: 1.8383\n","Epoch: 23/30... Step: 1800... Loss: 1.8276... Val Loss: 1.8223\n","Epoch: 24/30... Step: 1900... Loss: 1.8328... Val Loss: 1.8135\n","Epoch: 25/30... Step: 2000... Loss: 1.6498... Val Loss: 1.8171\n","Epoch: 27/30... Step: 2100... Loss: 1.7364... Val Loss: 1.8102\n","Epoch: 28/30... Step: 2200... Loss: 1.7424... Val Loss: 1.8013\n","Epoch: 29/30... Step: 2300... Loss: 1.7492... Val Loss: 1.7898\n","Epoch: 30/30... Step: 2400... Loss: 1.5874... Val Loss: 1.7965\n"]}]},{"cell_type":"code","source":["torch.save(net, '/content/drive/Shareddrives/LovePoem/baselineModel')"],"metadata":{"id":"DHhaxRZNj51P","executionInfo":{"status":"ok","timestamp":1701824634410,"user_tz":480,"elapsed":313,"user":{"displayName":"Xingshuo Xiao","userId":"07626625558595921239"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["def predict(net, char, h=None, top_k=None):\n","        ''' Given a character, predict the next character.\n","            Returns the predicted character and the hidden state.\n","        '''\n","\n","        # tensor inputs\n","        x = np.array([[net.char2int[char]]])\n","        x = one_hot_encode(x, len(net.chars))\n","        inputs = torch.from_numpy(x)\n","\n","        if(train_on_gpu):\n","            inputs = inputs.cuda()\n","\n","        # detach hidden state from history\n","        h = tuple([each.data for each in h])\n","        # get the output of the model\n","        out, h = net(inputs, h)\n","        # get the character probabilities\n","        p = F.softmax(out, dim=1).data\n","        if(train_on_gpu):\n","            p = p.cpu() # move to cpu\n","\n","        # get top characters\n","        if top_k is None:\n","            top_ch = np.arange(len(net.chars))\n","        else:\n","            p, top_ch = p.topk(top_k)\n","            top_ch = top_ch.numpy().squeeze()\n","\n","        # select the likely next character with some element of randomness\n","        p = p.numpy().squeeze()\n","        char = np.random.choice(top_ch, p=p/p.sum())\n","\n","        # return the encoded value of the predicted char and the hidden state\n","        return net.int2char[char], h\n","def sample(net, size, prime='The', top_k=None):\n","\n","    if(train_on_gpu):\n","        net.cuda()\n","    else:\n","        net.cpu()\n","\n","    net.eval() # eval mode\n","\n","    # First off, run through the prime characters\n","    chars = [ch for ch in prime]\n","    h = net.init_hidden(1)\n","    for ch in prime:\n","        char, h = predict(net, ch, h, top_k=top_k)\n","    chars.append(char)\n","\n","    # Now pass in the previous character and get a new one\n","    for ii in range(size):\n","        char, h = predict(net, chars[-1], h, top_k=top_k)\n","        chars.append(char)\n","    return ''.join(chars)"],"metadata":{"id":"gSyIkiDVhI5z","executionInfo":{"status":"ok","timestamp":1701824637805,"user_tz":480,"elapsed":263,"user":{"displayName":"Xingshuo Xiao","userId":"07626625558595921239"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["print(sample(net, 5000, prime='Love', top_k = 2))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qJ6dcFVkiF1E","executionInfo":{"status":"ok","timestamp":1701824642516,"user_tz":480,"elapsed":3866,"user":{"displayName":"Xingshuo Xiao","userId":"07626625558595921239"}},"outputId":"7d9e835a-cb80-4b4f-db85-0c4269701d3c"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Love to me\n","\n","In an my harl that stars of my lone \n","I love your say \n","I said the sky of me the bed \n","I love thee word a song to me\n","And the was and shall that have the bed\n","\n","In a song the brows and some there\n","I love to me a dow to than shall me to me \n","And to to more to the soft to more\n","\n","The shade they beat to the wild will shall breath and to me\n","\n","The stars a seaden the beet of the song\n","A love a seading and star a dow\n","\n","The songe the beat the way of stall that\n","When the stillen to the song and to be\n","\n","I love thee to the song that\n","I love the softed to me \n","\n","I love thee to me to me touck to me\n","\n","In the soft and there the brows and song\n","And shadow and she with me\n","\n","I live the seeting there the stars\n","I love it the houth to the song\n","\n","The sky to to she with the bed that some\n","The shalow the song the wild with me and the brows\n","And shall still be and that some and stalled to this beaut\n","And the stars a song the soft\n","I shall the seath and the soul to may\n","\n","I shall be to she the soully betoren ther\n","And that the beauty\n","I love the seadow the way\n","I said and the stars and the beauty\n","\n","I shall to more the stars and the streat\n","And to the stored the bed and song <|endoftext|>\n","\n","I was an my love is so more and so day \n","And the world to me the breath\n","That she wat the song\n","I love the seeting a some\n","The stirs a seemed the beatted to to more\n","And I with me and she liking the beat to soud\n","And the will be and the song and there the strange\n","And the soul the beat and that the bed\n","\n","The wanter and starse and the streat\n","And who he star to me a litter some to morning to me there and star and so the world \n","\n","The ways and stall and the bed\n","\n","I was a song and so day and to the beat\n","And so that sould the breath\n","A love that some that stall\n","\n","I love the song to me \n","\n","The ske and to me a love a little some <|endoftext|>\n","\n","I will the soul a day\n","And that to the bed that heart <|endoftext|> \n","That’s my life my love a love\n","\n","The stare a stare that stars\n","And the song to me and the store\n","The sun of my love and so thing of my hand\n","A love the song a love a seeped to thing \n","\n","I love thee that the mares and star of the brow\n","I love your hand and some and the store\n","The ways and shadow a love the stars\n","In the some to me and that stars\n","And that she she she was a stall and stars\n","And whene’s may broom to me and the soulless\n","\n","I san with me and the bed the star of me\n","\n","I love that soul a street\n","I love the soft the soft the beat\n","Ind the more and street the beat \n","And that the bed the beed and soulled\n","I shall be me that some and that to shall\n","And to more to that some\n","And the word the breath to me\n","And with the song that streed\n","I sang the soft and star and so long\n","And she was that’s moon and the beautifle the stars\n","And the wind was a soully to to me <|endoftext|>\n","\n","I love thee to me and the body \n","I love thee with the morning so love\n","That soul the song the song and some\n","To love the moon the world the soft to moon\n","\n","The she will be the stars and stalled tomere\n","The shadow and sould to the bed \n","I love thee to me a love the song\n","And with my life a street and soul and the brow\n","I was and to to dow the store to more \n","I said and sould the wind wan some\n","The sunding the song\n","I shall the seeming to to me\n","\n","The wind a love the song the star of the street\n","And the song the song a light\n","And to that the song\n","I shall the seadow\n","And the way of the brows\n","A like a seaden and the sead the brows\n","And the star a stall the broom\n","\n","I like the word to more\n","The song a love’s song to the breath\n","I sain the sound a sone to me\n","A song a song to more\n","The song the stall and street\n","A love that was an ment a streath\n","And the wander to me\n","I sholles of this hand and star\n","If an the see and she was the sofless of me\n","\n","The shade where tought the brown the stread\n","\n","I said the sky of the like the stall the stars\n","And the stroull stirl and sour to me\n","And the ways and stald and that to she with my heart\n","I love thas the streng and stars\n","And the will wan the streed\n","\n","The song an me the star and the song\n","I love you tould be and the star of to so dear\n","\n","I love the seething\n","I world the stars one the strain <|endoftext|>\n","\n","I love thee the sof the soft that soul that strange\n","And the softer a ligtle with the brow\n","Ind when I wish a love that the strang of this\n","While wat with the way of meant and song <|endoftext|>\n","\n","I love that would see the song\n","And that the bed\n","In a sould the brows <|endoftext|>\n","\n","I like the song and the song\n","I sand the soft and somane\n","I sange that the stars of the star\n","And that she will the strong to the stars\n","I love that with the song to me \n","I love the how a day\n","I love thee with the stars\n","I love the song the stars\n","And the soul to the song\n","\n","The sked and seaden the song\n","In the mest of love a sought\n","And I wat the wind wan than the song the street\n","And the song the stall the song\n","\n","The ways that to steal a love to me <|endoftext|>\n","\n","The stars and stars a song the star\n","And she the light of the song the song\n","\n","I was an the brown to the song and song <|endoftext|>\n","\n","I wish a light and stars and the bread\n","I san the bread\n","I say the star to there and stant\n","And s\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"S7YtbW9hiILU"},"execution_count":null,"outputs":[]}]}